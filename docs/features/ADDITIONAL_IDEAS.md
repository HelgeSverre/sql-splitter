# Additional Feature Ideas

**Date**: 2025-12-20  
**Updated**: 2025-12-20  
**Source**: Oracle analysis

Additional high-value features beyond the current roadmap (split, merge, analyze, sample, shard, query, redact, diff, convert).

> **Note:** Dependency-aware sampling and shard-by-key have been moved to their own feature documents:
> - [Sample Feature](SAMPLE_FEATURE.md) — includes `--preserve-relations` for FK-aware sampling
> - [Shard Feature](SHARD_FEATURE.md) — tenant extraction with FK chain resolution

---

## 1. Canonicalize + Checksum

Normalize SQL formatting and compute strong checksums for CI verification and artifact deduplication.

```bash
# Canonicalize and compute checksums
sql-splitter canonicalize dump.sql -o normalized.sql --checksum

# Per-table checksums for granular comparison
sql-splitter canonicalize dump.sql --checksums-only --format json > checksums.json

# Verify a dump against known checksums
sql-splitter verify dump.sql --checksums checksums.json
```

**Normalization:**
- Consistent identifier quoting (per dialect)
- Consistent keyword casing (uppercase/lowercase)
- Normalized whitespace
- Sorted table order (alphabetical)
- Optionally sorted rows by primary key

**Checksum output:**
```json
{
  "file_hash": "sha256:a1b2c3...",
  "tables": {
    "users": {"hash": "sha256:d4e5f6...", "rows": 1234},
    "orders": {"hash": "sha256:g7h8i9...", "rows": 5678}
  }
}
```

**Use cases:**
- CI pipeline: "Is this dump semantically identical to the reference?"
- Artifact deduplication in storage
- Quick comparison without full diff
- Verify dump integrity after transfer

**Effort:** M (2-3 days)

---

## 2. Auto-Detect PII (`detect-pii`)

Scan schema and sample rows to automatically suggest a redaction configuration.

```bash
# Analyze and suggest redaction rules
sql-splitter detect-pii dump.sql -o redact-config.yaml

# Preview detections without writing config
sql-splitter detect-pii dump.sql --preview

# Scan only schema (no data sampling)
sql-splitter detect-pii dump.sql --schema-only

# Pipe directly to redact
sql-splitter detect-pii dump.sql | sql-splitter redact dump.sql --config - -o safe.sql
```

**Detection heuristics:**

| Pattern | Columns Detected |
|---------|-----------------|
| Column names | `email`, `mail`, `phone`, `tel`, `ssn`, `tax_id`, `dob`, `birth`, `address`, `zip`, `password`, `secret`, `token` |
| Data patterns | Email regex, phone formats, SSN format, credit card Luhn, IBAN structure |
| Statistical | High uniqueness + specific length distributions |

**Output config:**
```yaml
# Auto-generated by sql-splitter detect-pii
# Review and adjust before using with redact

detected_columns:
  - column: users.email
    confidence: high
    reason: "column name 'email' + valid email pattern in 98% of rows"
    suggested_strategy: fake_email
    
  - column: users.ssn
    confidence: high
    reason: "column name 'ssn' + SSN pattern (###-##-####)"
    suggested_strategy: "null"
    
  - column: orders.customer_phone
    confidence: medium
    reason: "column name contains 'phone'"
    suggested_strategy: fake_phone
```

**Use cases:**
- Bootstrap redaction config for unfamiliar databases
- Audit databases for PII exposure
- Compliance scanning without DB access

**Effort:** S-M (1-2 days)

---

## 3. Validate/Lint (`validate`)

Check dump files for structural integrity and common issues.

```bash
# Full validation
sql-splitter validate dump.sql

# Quick syntax check only
sql-splitter validate dump.sql --syntax-only

# Validate with FK integrity check
sql-splitter validate dump.sql --check-fk

# Output as JSON for CI
sql-splitter validate dump.sql --format json > validation.json
```

**Checks performed:**

| Category | Checks |
|----------|--------|
| **Syntax** | Truncated statements, unclosed strings, mismatched parens |
| **DDL/DML consistency** | INSERT references existing table, column counts match |
| **Data integrity** | Duplicate primary keys, NULL in NOT NULL columns |
| **FK integrity** | Referenced keys exist (optional, requires full scan) |
| **Format** | Character encoding issues, binary data in text columns |

**Output:**
```
Validating: dump.sql (1.2 GB)

✓ Syntax: OK
✓ DDL/DML consistency: OK
⚠ Data integrity: 2 warnings
  - Table 'orders': 3 duplicate primary keys (ids: 1001, 1002, 1003)
  - Table 'users': NULL in NOT NULL column 'email' (row ~45000)
✗ FK integrity: 1 error
  - Table 'orders': 15 rows reference non-existent user_id

Summary: 0 errors, 2 warnings (use --strict to fail on warnings)
```

**Use cases:**
- Pre-import validation in ETL pipelines
- CI gate for dump artifacts
- Debug corrupted/truncated dumps
- Verify backup integrity

**Effort:** M-L (2-4 days)

---

## 4. Key Range Partitioning

Extend the shard command with date/time range partitioning for temporal data.

```bash
# Partition by date ranges
sql-splitter shard dump.sql -o ranges/ \
  --key created_at \
  --ranges "2023-01-01,2024-01-01,2025-01-01"

# Monthly partitions
sql-splitter shard dump.sql -o monthly/ \
  --key order_date \
  --interval month \
  --start 2024-01-01 \
  --end 2025-01-01
```

**Output structure:**
```
ranges/
├── before_2023-01-01.sql
├── 2023-01-01_to_2024-01-01.sql
├── 2024-01-01_to_2025-01-01.sql
└── after_2025-01-01.sql
```

**Use cases:**
- Archive old data separately
- Time-based data retention
- Faster partial restores

**Effort:** M (2-3 days, builds on shard infrastructure)

---

## 5. Streaming Output Compression

Output directly to compressed formats without intermediate files.

```bash
# Output to gzip
sql-splitter split dump.sql -o tables/ --compress gzip

# Output to zstd (faster, better compression)
sql-splitter sample dump.sql -o sample.sql.zst --compress zstd

# Specify compression level
sql-splitter shard dump.sql -o tenant.sql.gz --compress gzip --level 9
```

**Supported formats:**
- gzip (`.gz`)
- zstd (`.zst`) — recommended for speed + ratio
- bzip2 (`.bz2`)
- xz (`.xz`)

**Effort:** S (1 day, extend existing compression infrastructure)

---

## Priority Matrix

| Feature | Value | Complexity | Recommendation |
|---------|-------|------------|----------------|
| **Core roadmap:** |
| Merge | High | Low | ✅ Implemented |
| Sample | High | Medium | Next priority |
| Shard | High | Medium-High | After sample (shares FK graph) |
| Redact | High | Medium-High | High value |
| Query | Medium | Medium-High | Nice to have |
| Convert | Medium | High | Complex |
| Diff | Medium | High | Later |
| **Additional ideas:** |
| Validate | High | Medium | **Easy CI win** |
| Detect-PII | High | Low-Medium | **Pairs with redact** |
| Canonicalize | Medium | Medium | CI/verification |
| Key Range Partitioning | Medium | Low | Builds on shard |
| Streaming Compression | Medium | Low | Quick win |

---

## Shared Infrastructure

Several features share common components:

### Schema Graph (FK Parsing)

Used by:
- `sample --preserve-relations`
- `shard`
- `validate --check-fk`
- `diff` (structural comparison)

### Row Parsing (INSERT/COPY values)

Used by:
- `sample` (PK/FK extraction)
- `shard` (tenant column + FK values)
- `redact` (column value replacement)
- `detect-pii` (data pattern analysis)
- `validate` (PK uniqueness, NOT NULL checks)

### PK Tracking

Used by:
- `sample --preserve-relations`
- `shard`
- `validate --check-fk`

**Implementation strategy:** Build these as reusable modules in `src/schema/` and `src/row/`.

---

## Related

- [Sample Feature](SAMPLE_FEATURE.md) — FK-aware sampling with `--preserve-relations`
- [Shard Feature](SHARD_FEATURE.md) — Tenant extraction with FK chain resolution
- [Redact Feature](REDACT_FEATURE.md) — Data anonymization
- [MSSQL Support Analysis](MSSQL_FEASIBILITY.md) — T-SQL dialect support
