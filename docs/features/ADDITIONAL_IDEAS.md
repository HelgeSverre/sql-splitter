# Additional Feature Ideas

**Date**: 2025-12-20  
**Source**: Oracle analysis

Five additional high-value features beyond the current roadmap (split, merge, analyze, sample, query, redact, diff, convert).

---

## 1. Dependency-Aware Sampling (`sample --preserve-relations`)

**Already partially planned in SAMPLE_FEATURE.md, but worth emphasizing as distinct capability.**

Enhanced sampling that preserves referential integrity across the entire database graph.

```bash
# Sample 1% of orders, automatically include all referenced customers, products, etc.
sql-splitter sample dump.sql -o subset.sql --percent 1 --preserve-relations

# Start from specific "root" tables
sql-splitter sample dump.sql -o subset.sql --root-tables orders --percent 5 --preserve-relations
```

**Algorithm:**
1. Parse CREATE TABLE statements to build FK dependency graph
2. Sample rows from root/specified tables
3. Track selected primary key values in hash sets
4. Include all referenced rows from dependent tables
5. Recursively follow FK chains

**Why unique:** Most tools sample tables independently; maintaining a consistent relational slice is rare.

---

## 2. Shard/Partition by Key

Split table data into multiple dumps by tenant, hash, or key range — for multi-tenant systems and parallel loading.

```bash
# Partition by tenant_id into separate dumps
sql-splitter shard dump.sql -o shards/ --key tenant_id --partitions 10

# Hash-based sharding for parallel load
sql-splitter shard dump.sql -o chunks/ --key id --hash --partitions 8

# Key range partitioning
sql-splitter shard dump.sql -o ranges/ --key created_at --ranges "2023-01-01,2024-01-01,2025-01-01"

# Explicit mapping file (tenant_id -> shard)
sql-splitter shard dump.sql -o tenants/ --key tenant_id --mapping tenant_shards.json
```

**Output structure:**
```
shards/
├── shard_0/
│   ├── users.sql
│   ├── orders.sql
│   └── ...
├── shard_1/
│   └── ...
└── schema.sql  # Shared DDL (optional)
```

**Use cases:**
- Split multi-tenant monolith dump into per-tenant dumps
- Prepare N balanced chunks for parallel bulk loading
- Migrate specific tenants to different databases

**Effort:** M (2-3 days)

---

## 3. Canonicalize + Checksum

Normalize SQL formatting and compute strong checksums for CI verification and artifact deduplication.

```bash
# Canonicalize and compute checksums
sql-splitter canonicalize dump.sql -o normalized.sql --checksum

# Per-table checksums for granular comparison
sql-splitter canonicalize dump.sql --checksums-only --format json > checksums.json

# Verify a dump against known checksums
sql-splitter verify dump.sql --checksums checksums.json
```

**Normalization:**
- Consistent identifier quoting (per dialect)
- Consistent keyword casing (uppercase/lowercase)
- Normalized whitespace
- Sorted table order (alphabetical)
- Optionally sorted rows by primary key

**Checksum output:**
```json
{
  "file_hash": "sha256:a1b2c3...",
  "tables": {
    "users": {"hash": "sha256:d4e5f6...", "rows": 1234},
    "orders": {"hash": "sha256:g7h8i9...", "rows": 5678}
  }
}
```

**Use cases:**
- CI pipeline: "Is this dump semantically identical to the reference?"
- Artifact deduplication in storage
- Quick comparison without full diff
- Verify dump integrity after transfer

**Effort:** M (2-3 days)

---

## 4. Auto-Detect PII (`detect-pii`)

Scan schema and sample rows to automatically suggest a redaction configuration.

```bash
# Analyze and suggest redaction rules
sql-splitter detect-pii dump.sql -o redact-config.yaml

# Preview detections without writing config
sql-splitter detect-pii dump.sql --preview

# Scan only schema (no data sampling)
sql-splitter detect-pii dump.sql --schema-only

# Pipe directly to redact
sql-splitter detect-pii dump.sql | sql-splitter redact dump.sql --config - -o safe.sql
```

**Detection heuristics:**

| Pattern | Columns Detected |
|---------|-----------------|
| Column names | `email`, `mail`, `phone`, `tel`, `ssn`, `tax_id`, `dob`, `birth`, `address`, `zip`, `password`, `secret`, `token` |
| Data patterns | Email regex, phone formats, SSN format, credit card Luhn, IBAN structure |
| Statistical | High uniqueness + specific length distributions |

**Output config:**
```yaml
# Auto-generated by sql-splitter detect-pii
# Review and adjust before using with redact

detected_columns:
  - column: users.email
    confidence: high
    reason: "column name 'email' + valid email pattern in 98% of rows"
    suggested_strategy: fake_email
    
  - column: users.ssn
    confidence: high
    reason: "column name 'ssn' + SSN pattern (###-##-####)"
    suggested_strategy: "null"
    
  - column: orders.customer_phone
    confidence: medium
    reason: "column name contains 'phone'"
    suggested_strategy: fake_phone
```

**Use cases:**
- Bootstrap redaction config for unfamiliar databases
- Audit databases for PII exposure
- Compliance scanning without DB access

**Effort:** S-M (1-2 days)

---

## 5. Validate/Lint (`validate`)

Check dump files for structural integrity and common issues.

```bash
# Full validation
sql-splitter validate dump.sql

# Quick syntax check only
sql-splitter validate dump.sql --syntax-only

# Validate with FK integrity check
sql-splitter validate dump.sql --check-fk

# Output as JSON for CI
sql-splitter validate dump.sql --format json > validation.json
```

**Checks performed:**

| Category | Checks |
|----------|--------|
| **Syntax** | Truncated statements, unclosed strings, mismatched parens |
| **DDL/DML consistency** | INSERT references existing table, column counts match |
| **Data integrity** | Duplicate primary keys, NULL in NOT NULL columns |
| **FK integrity** | Referenced keys exist (optional, requires full scan) |
| **Format** | Character encoding issues, binary data in text columns |

**Output:**
```
Validating: dump.sql (1.2 GB)

✓ Syntax: OK
✓ DDL/DML consistency: OK
⚠ Data integrity: 2 warnings
  - Table 'orders': 3 duplicate primary keys (ids: 1001, 1002, 1003)
  - Table 'users': NULL in NOT NULL column 'email' (row ~45000)
✗ FK integrity: 1 error
  - Table 'orders': 15 rows reference non-existent user_id

Summary: 0 errors, 2 warnings (use --strict to fail on warnings)
```

**Use cases:**
- Pre-import validation in ETL pipelines
- CI gate for dump artifacts
- Debug corrupted/truncated dumps
- Verify backup integrity

**Effort:** M-L (2-4 days)

---

## Priority Matrix (Updated)

| Feature | Value | Complexity | Recommendation |
|---------|-------|------------|----------------|
| **Existing planned:** |
| Merge | High | Low | Do first |
| Sample | High | Medium | Do second |
| Redact | High | Medium-High | High value |
| Query | Medium | Medium-High | Nice to have |
| Convert | Medium | High | Complex |
| Diff | Medium | High | Later |
| **New ideas:** |
| Validate | High | Medium | **Easy win for CI** |
| Detect-PII | High | Low-Medium | **Pairs with redact** |
| Shard | Medium | Medium | Multi-tenant use case |
| Canonicalize | Medium | Medium | CI/verification |
| Dependency sampling | Medium | High | Already in sample plan |

---

## Related

- [Sample Feature](SAMPLE_FEATURE.md)
- [Redact Feature](REDACT_FEATURE.md)
- [MSSQL Support Analysis](MSSQL_FEASIBILITY.md)
